{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1g7SS6DF4ZuRKtoHsdbeQ3XfIQjcBd_Cm",
      "authorship_tag": "ABX9TyNlzKo9nlMz5KFO8blQYW9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naina-j04/Atri-Assignment/blob/main/TensorRT_Conversion_and_Inference_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TDOOcPLTCM6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBi3djwQitKz",
        "outputId": "45dacb32-58ba-47a7-a408-ced97909ad13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 256, 256, device=\"cuda\")\n",
        "torch.onnx.export(model, dummy_input, \"garbage.onnx\", opset_version=11)\n"
      ],
      "metadata": {
        "id": "lYbQGKIT2I5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82138d05-30d5-4266-bf2c-b8c676c5f89e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3575621061.py:2: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(model, dummy_input, \"garbage.onnx\", opset_version=11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt\n",
        "!pip install onnxruntime-gpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9xaQtHUkLAB",
        "outputId": "51ce23b8-cee4-41e8-d3c1-3c9b7e63086f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.12/dist-packages (10.13.3.9)\n",
            "Requirement already satisfied: tensorrt-cu13==10.13.3.9 in /usr/local/lib/python3.12/dist-packages (from tensorrt) (10.13.3.9)\n",
            "Requirement already satisfied: tensorrt-cu13-libs==10.13.3.9 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu13==10.13.3.9->tensorrt) (10.13.3.9)\n",
            "Requirement already satisfied: tensorrt-cu13-bindings==10.13.3.9 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu13==10.13.3.9->tensorrt) (10.13.3.9)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu13 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu13-libs==10.13.3.9->tensorrt-cu13==10.13.3.9->tensorrt) (0.0.0a0)\n",
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.12/dist-packages (1.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.13.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "session = ort.InferenceSession(\n",
        "    \"garbage.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "print(\"Input name:\", dummy_input)\n",
        "\n",
        "dummy_input = np.random.randn(1, 3, 256, 256).astype(np.float32)\n",
        "outputs = session.run(None, {input_name: dummy_input})\n",
        "print(\"Inference done, output shape:\", outputs[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuB6ihaDpXCf",
        "outputId": "1552c3a1-242e-4470-c05d-7b6e24d7b967"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input name: tensor([[[[-1.4861, -0.5077, -1.1385,  ..., -1.7409, -0.7173,  0.8181],\n",
            "          [-0.5270, -0.1506, -1.7448,  ..., -1.2186, -1.4222, -0.9324],\n",
            "          [ 0.2534, -1.2258, -0.4874,  ...,  0.4911,  2.0538, -0.1398],\n",
            "          ...,\n",
            "          [ 0.4108,  0.5042, -0.6240,  ..., -0.3623, -2.5194, -0.1014],\n",
            "          [-0.3738,  1.1263,  0.1817,  ...,  1.0988, -0.8465,  0.8414],\n",
            "          [ 0.7424,  0.2869, -0.9102,  ...,  1.1698, -1.1110,  0.7320]],\n",
            "\n",
            "         [[-0.2296, -0.2599,  0.8078,  ..., -0.8449,  0.1707, -2.1808],\n",
            "          [ 2.3263,  1.3296, -0.0918,  ...,  0.3316, -0.3282, -1.1878],\n",
            "          [ 0.6877,  0.8995,  0.2083,  ...,  0.6140, -0.2810, -0.4294],\n",
            "          ...,\n",
            "          [ 1.4194,  0.9867, -0.9599,  ..., -0.0082, -1.0392, -1.5934],\n",
            "          [-0.2457,  0.7716,  0.8490,  ...,  0.8379,  0.3335, -0.3807],\n",
            "          [-0.6915, -0.1015,  1.2668,  ...,  1.4560,  0.9314, -1.0864]],\n",
            "\n",
            "         [[ 1.1179, -1.2905, -0.7018,  ...,  0.5777,  0.3957, -0.6929],\n",
            "          [ 0.5541, -0.0924, -0.2328,  ...,  0.2575,  0.0855,  2.2485],\n",
            "          [ 1.2364, -1.0705,  1.0818,  ...,  1.5160, -0.3944, -0.3350],\n",
            "          ...,\n",
            "          [-1.1332, -1.3255, -1.0292,  ..., -0.5448,  0.4728, -2.0773],\n",
            "          [-0.7198,  0.6097,  0.0855,  ..., -0.4547,  0.7635, -1.8772],\n",
            "          [-1.1736,  0.5524,  0.7685,  ...,  0.1036, -1.3192, -0.5866]]]])\n",
            "Inference done, output shape: (1, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50()\n",
        "path = '/content/drive/MyDrive/Garbage_classification/Data/model_resnet1.pth'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "\n",
        "#Pytorch CPU\n",
        "model.cpu()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    output_cpu = model(dummy_input)\n",
        "cpu_time = time.time() - start\n",
        "cpu_latency = cpu_time / batch_size\n",
        "cpu_throughput = batch_size / cpu_time\n",
        "print(f\"PyTorch CPU: {cpu_latency:.4f}s/img, {cpu_throughput:.2f} img/s\")\n",
        "\n",
        "# Pytorch GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "dummy_input_gpu = dummy_input.to(device)\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    output_gpu = model(dummy_input_gpu)\n",
        "torch.cuda.synchronize()\n",
        "gpu_time = time.time() - start\n",
        "gpu_latency = gpu_time / batch_size\n",
        "gpu_throughput = batch_size / gpu_time\n",
        "print(f\"PyTorch GPU: {gpu_latency:.4f}s/img, {gpu_throughput:.2f} img/s\")\n",
        "\n",
        "\n",
        "# TensorRT\n",
        "session = ort.InferenceSession(\n",
        "    \"garbage.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "dummy_input_np = dummy_input.numpy()\n",
        "\n",
        "start = time.time()\n",
        "outputs_trt = session.run(None, {input_name: dummy_input_np})\n",
        "trt_time = time.time() - start\n",
        "trt_latency = trt_time / batch_size\n",
        "trt_throughput = batch_size / trt_time\n",
        "print(f\"TensorRT Model: {trt_latency:.4f}s/img, {trt_throughput:.2f} img/s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auq_HvCNrCXw",
        "outputId": "a27c9387-3015-42ef-96a3-dcd08ed87d9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CPU: 0.0292s/img, 34.26 img/s\n",
            "PyTorch GPU: 0.0009s/img, 1092.27 img/s\n",
            "TensorRT Model: 0.0055s/img, 181.39 img/s\n"
          ]
        }
      ]
    }
  ]
}